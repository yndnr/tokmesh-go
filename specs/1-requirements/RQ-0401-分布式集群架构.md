# RQ-0401-分布式集群架构

**状态**: 已批准
**优先级**: P1
**来源**: CP-0401-分布式集群架构.md, CP-0402-系统可观测性.md, CP-0502-运维管理体系.md
**最后更新**: 2025-12-17

## 1. 分布式集群

### 1.1 节点角色
- **对等架构 (Peer-to-Peer)**: 所有节点均可处理读写请求。
- **数据分片**: 
    - **算法**: **一致性哈希 (Consistent Hashing)**。
    - **哈希函数**: **MurmurHash3**。
    - **虚拟节点**: 每个物理节点对应 **256 个** 虚拟节点 (Virtual Nodes)，以解决数据倾斜。
    - **分片键**: `Hash(SessionID)`。
    - **迁移策略**: 
        - 当节点变动时，系统触发 Rebalance。
        - **数据搬迁优化**: Rebalance 过程将分批次进行，并对带宽进行**流控**（默认限制 20Mbps，可通过 `cluster.rebalance.max_rate` 配置，支持 `"100MBps"`、`"1Gbps"` 等格式；值格式见 `specs/1-requirements/RQ-0502-配置管理需求.md` 2.4.3）。
        - **一致性保证**: 迁移过程中，受影响分片的读写请求必须被正确路由（例如：旧节点在搬迁期间将请求代理至新节点，或返回 TryAgain）。
        - **数据完整性**: 确保在迁移完成前，旧节点数据不删除；迁移完成后，新节点数据立即可用。
        - **TTL 优化**: 在数据搬迁时，会跳过即将过期的 Session (默认 < 60s，可通过 `cluster.rebalance.min_ttl` 配置，支持 Duration 格式如 "60s")。

### 1.2 节点发现
- **机制**: 采用基于 **Gossip 协议** 的动态节点发现。
- **配置**: `cluster.discovery.seeds` 列表。
- **集群标识 (Cluster ID)**:
    - 集群初始化时生成唯一 UUID。
    - **安全防护**: 节点握手时必须校验 Cluster ID。
    - **冲突处理**: 若发现对方 Cluster ID 与本地已持久化的 ID 不一致，**拒绝连接并告警**。这防止了两个因网络分区独立形成的集群在网络恢复后错误合并。
- **启动逻辑**:
    - **单机模式**: 如果 `cluster.discovery.seeds` 为空且未配置 `cluster.bootstrap.expect_nodes`，作为首个节点启动。
    - **集群模式**:
        1.  **连接种子**: 尝试连接 `cluster.discovery.seeds` 列表中的节点。
        2.  **场景 A: 加入现有集群**:
            - 如果连接到的种子节点通过 Gossip 返回了已存在的 `cluster_id` 和集群拓扑。
            - 节点自动发起加入请求 (Join Request)，同步元数据并开始工作。
        3.  **场景 B: 全新集群初始化**:
            - 如果连接到的种子节点均为“待初始化”状态。
            - 节点进入**等待状态**，直到通过 Gossip 互相发现的节点数量达到 **`cluster.bootstrap.expect_nodes`** (配置项，例如 3)。
            - 一旦满足 `available_nodes >= cluster.bootstrap.expect_nodes`，节点间自动根据确定性规则（如 NodeID 最小者）发起 Raft 初始化引导。
        - **并行启动支持**: 节点在无法立即连接种子时会持续重试（默认超时 5 分钟），支持所有节点同时启动。

### 1.3 混合架构实施路径 (Hybrid Architecture Implementation)
TokMesh 采用分层架构以兼顾元数据的强一致性和业务数据的高吞吐量。

#### 1.3.1 控制面 (Control Plane) - Raft
- **适用数据**: 集群拓扑 (Cluster Topology)、分片映射表 (Shard Map)、API Key 策略、系统全局配置。
- **实施机制**: 
    - 集群内部维护一个轻量级 Raft Group。
    - 所有元数据变更（如节点加入、Key 创建）必须提交到 Raft Leader。

##### 1.3.1.1 偶数节点风险提示 (Even-Numbered Cluster Warning)
- **检测时机**: 启动时检查及运行时节点变更后检查。
- **告警行为**:
    - **日志**: `WARN: Cluster has even number of nodes (4). Network partition may cause quorum loss.`
    - **Metrics**: `tokmesh_cluster_nodes_parity` = 1（偶数），0（奇数）
    - **Dashboard**: 显示橙色警告标志。
    - **tokmesh-cli status**: 输出 `Health: DEGRADED (Even nodes: 4)`。
- **允许运行**: 系统不阻止偶数节点启动，但会持续告警直到调整为奇数。
- **推荐配置**: 生产环境强烈建议 **3, 5, 7** 个节点以获得最佳容错性 (N/2+1)。避免 2, 4, 6 节点。

#### 1.3.2 数据面 (Data Plane) - Consistent Hashing
- **适用数据**: 高频的 Session / Token 数据。
- **实施机制**: 
    - **不走 Raft Log**: 业务数据的读写**不经过** Raft 共识层，以避免单点瓶颈。
    - **路由**: 客户端/代理端根据 Raft 同步下来的 Shard Map，将请求直接发往 Owner 节点。
    - **复制策略**: 
        - **参数**: `cluster.data.replication_factor` (Integer, Default: **1**)。管理员可根据硬件资源和容灾需求灵活配置 (e.g., 2, 3)。
        - **约束**: 
            - **配置不合理警告**: 若配置值大于集群节点总数 (`cluster.data.replication_factor > node_count`)，系统将发出 **"Replication factor exceeds cluster size"** 告警，并自动将该分片的实际有效副本数限制为 `node_count`。
            - **运行时副本监测 (Under-replicated)**: 若某个分片 (Shard) 的**实际存活副本数**低于配置的 `cluster.data.replication_factor` 值（例如：配置 RF=3，但当前只有 2 个副本存活），系统将触发 **`Under-replicated`** 告警。
        - **默认模式**: **异步复制 (Async Replication)**。Owner 节点将数据异步复制到 Backup 节点。
        - **一致性权衡 (RPO)**: 
            - 此策略**优先保障写入性能**。RPO 取决于异步复制延迟 (Replication Lag)。
            - **监控**: 若副本滞后超过阈值 (如 1s)，应触发 `Replication Lag` 告警。
        - **可选 (同步复制)**: 
            - 对于极度敏感场景，未来可提供**同步复制 (Sync Replication)**。
            - **约束**: 若开启同步复制，必须强制 `cluster.data.replication_factor` 为奇数 (3, 5...) 以确保副本间共识，防止脑裂。

#### 1.3.3 监测面 (Monitoring Plane) - Gossip
- **适用数据**: 节点健康状态 (Liveness)、负载指标 (Load Info)。
- **实施机制**: 
    - 节点间高频交换 Gossip 心跳。
    - 当 Gossip 检测到节点疑似下线 (Pfail) 并经确认 (Fail) 后，触发控制面 (Raft) 进行拓扑更新和重新分片 (Rebalance)。

## 2. 可观测性 (Observability)
- **目的**: 确保在集群环境下，TokMesh 节点的运行状态、性能指标、日志等信息能够被全面监控。
- **详见**: [RQ-0403-可观测性需求.md]

## 3. 运维工具 (tokmesh-cli)
- **目的**: 提供集群级别的管理与控制能力。
- **详见**: [RQ-0304-管理接口规约.md]

### 3.1 命令集 (Cluster Specific)
- `tokmesh-cli cluster status`: 查看集群拓扑、节点健康状况、分片分布。
- `tokmesh-cli cluster rebalance`: 手动触发数据分片再平衡。
- `tokmesh-cli cluster leave --node-id=tmnd-xxx`: 强制移除失效节点。
- `tokmesh-cli cluster reset --force`: 强制重置当前节点（清空数据与状态），用于解决脑裂后的合并问题。

## 4. 验收标准 (Acceptance Criteria)

1.  **分片均衡**:
    - [ ] 3节点集群，插入 100万 Key，各节点数据量差异 < 10%。
2.  **弹性伸缩**:
    - [ ] 增加一个节点，系统自动触发 Rebalance，部分数据迁移到新节点。
    - [ ] 移除一个节点，该节点的数据被接管。
3.  **单机模式**:
    - [ ] 不配置 seeds 也能正常启动并提供服务。
4.  **容灾与恢复 (Partition Tolerance)**:
    - [ ] **脑裂保护**: 模拟网络分区将 5 节点集群切分为 2+3。验证少数派分区 (2节点) 自动降级为只读或拒绝服务，确保不产生脏数据。
    - [ ] **分区恢复**: 网络恢复后，少数派节点自动重新加入集群，并从多数派同步最新的分片拓扑。
    - [ ] **主备切换RPO**: 在一个 Primary-Backup 对中，模拟 Primary 突然宕机。验证 Backup 提升为 Primary 后，最新的 1 秒内写入的数据有**极小概率**丢失，但大部分数据完整。
    - [ ] **配置风险提示 (偶数节点)**: 启动一个 4 节点集群：
        - 日志中应包含 `WARN: Cluster has even number of nodes`。
        - Metrics `tokmesh_cluster_nodes_parity{type="even"}` 值为 1。
        - `tokmesh-cli cluster status` 输出显示 `Health: DEGRADED`。
        - **恢复验证**: 添加第 5 个节点后，上述告警应自动消失。
