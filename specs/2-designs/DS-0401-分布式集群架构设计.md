# DS-0401 - 分布式集群架构设计

**状态**: 已批准
**优先级**: P1
**来源**: RQ-0401-分布式集群架构.md, RQ-0402-性能与可靠性需求.md, specs/governance/charter.md
**作者**: Gemini
**创建日期**: 2025-12-12
**最后更新**: 2025-12-18

## 1. 概述

本文档详细阐述 TokMesh 分布式集群的整体架构设计。基于 RQ-0401-分布式集群架构.md 中定义的混合一致性模型，本设计旨在实现元数据强一致性、业务数据高吞吐量及集群高可用，并应对节点动态变化、网络分区等复杂场景。

## 2. 整体架构与分层

TokMesh 集群采用三层混合架构：

1.  **控制面 (Control Plane)**: 负责集群元数据管理，保障强一致性。
2.  **数据面 (Data Plane)**: 负责业务数据（Session/Token）的存储与访问，保障高性能。
3.  **监测面 (Monitoring Plane)**: 负责节点健康状态探测与故障发现，保障集群韧性。

### 2.1 架构分层图

```mermaid
graph TB
    subgraph 客户端层 (Client Layer)
        CLI[tokmesh-cli / HTTP Clients]
    end

    subgraph TokMesh 节点 (TokMesh Node)
        API[API Gateway]

        subgraph 控制面 (Control Plane)
            RAFT[Raft Consensus<br/>元数据管理]
        end

        subgraph 数据面 (Data Plane)
            HASH[Consistent Hashing<br/>路由引擎]
            SHARD[Local Data Shard<br/>内存存储]
            REP[Replication Manager<br/>异步复制]
        end

        subgraph 监测面 (Monitoring Plane)
            GOSSIP[Gossip Protocol<br/>健康探测]
        end
    end

    CLI -->|HTTP/Redis| API
    API --> HASH
    HASH -->|查询 Shard Map| RAFT
    HASH --> SHARD
    SHARD --> REP
    GOSSIP -->|节点状态| RAFT
    RAFT -->|集群状态| HASH

    REP -.->|异步复制| REMOTE[Remote Backup Node]
```

### 2.2 各层职责

*   **控制面 (Raft)**:
    *   **核心组件**: Raft Consensus Group。
    *   **职责**:
        *   维护集群元数据（Cluster Map, Shard Map, API Key 策略）。
        *   Leader 选举，Log 复制，快照。
        *   处理节点加入/退出时的拓扑变更。
    *   **一致性**: CP (强一致性)。
    *   **持久化**: Raft 日志 (Log) 和状态机 (FSM) 存储介质：嵌入式持久化存储（具体引擎在实现前通过 ADR 决策，参考 `specs/adrs/AD-0401-集群元数据持久化引擎选型策略.md` 与 `specs/adrs/AD-0402-嵌入式KV选型策略.md`）。
    *   **线性一致性读**: 通过 **Raft ReadIndex 机制** 或 **Leader Lease 机制** 实现，确保读取到的是最新的已提交数据，避免脏读。
    *   **快照与日志压缩**:
        - Raft 日志条目数达到 **10,000 条**（阈值可配置；Phase 3 实现时定义具体配置项）时，自动触发快照。
        - 快照包含完整的元数据状态机（Cluster Map, Shard Map, API Keys）。
        - 快照完成后，保留最近 **1,000 条**日志（阈值可配置；Phase 3 实现时定义具体配置项），旧日志被删除。
    *   **Raft Group 规模与角色 (Scalability)**:
        *   为支持大规模集群 (100+ 节点)，并非所有节点都参与投票。
        *   **Voters (投票成员)**: 固定数量（建议 3, 5 或 7 个），负责达成共识和选主。
        *   **Learners (观察者)**: 其余节点作为 Learners，不参与投票，仅从 Leader 异步复制日志以获取最新的元数据 (Shard Map)。这确保了 Raft 性能不会随集群规模线性下降。

*   **数据面 (Consistent Hashing)**:
    *   **核心组件**: Consistent Hashing Ring, Local Data Shard, Replication Manager。
    *   **职责**:
        *   根据 Shard Map 路由 Session / Token 读写请求。
        *   本地内存数据存储（Session Data）。
        *   异步复制数据到 Backup 节点。
    *   **一致性**:
    *   **单节点内**: 强一致（所有对同一 SessionID 的操作在 Owner 节点内串行化）。
    *   **副本间**: 最终一致（异步复制导致副本间存在秒级延迟）。

*   **监测面 (Gossip)**:
    *   **核心组件**: Gossip Protocol。
    *   **职责**:
        *   节点间 P2P 心跳交换，快速探测节点存活状态 (Liveness)。
        *   传播节点负载、版本等非关键信息。
        *   当检测到节点疑似故障时，通知控制面进行确认。
    *   **一致性**: AP (最终一致性)。

## 3. 集群引导 (Bootstrap) 与节点管理

### 3.1 节点配置 (Node Configuration)

*   **`cluster.node_id`**: 节点的唯一标识符。留空则自动生成并持久化（推荐：分发统一配置时不填写）。
*   **`cluster.listen_address`**: 集群内部服务的本机监听地址 (IP:Port)。
*   **`cluster.advertise_address`**: 节点对外宣告地址 (IP:Port)，必须为“其他节点可访问”的本机地址。
*   **`cluster.tls.*`**: 集群内部 mTLS 证书与 CA（启用集群时必需）。
*   **`cluster.discovery.seeds`**: 初始种子节点列表。
*   **`cluster.bootstrap.expect_nodes`**: 全新集群初始化时的预期节点数。
*   **`cluster_id`**: 集群唯一标识符，首次初始化时生成并持久化（**非配置项**，用于脑裂保护）。

**NodeID 自动生成与持久化（减少误用）**：
- 若 `cluster.node_id` 为空：启动时从本地持久化文件读取；若不存在则生成一次并写入，后续重启保持不变。
- 建议持久化到数据目录（Linux 服务化建议 `/var/lib/tokmesh-server/node-id`）。容器/Kubernetes 必须挂载持久化卷，否则 Pod 重建会导致 node_id 变化，集群成员视图频繁抖动。

**“统一配置文件分发”的推荐方式**：
- 配置文件不填写 `cluster.node_id`，由服务端自动生成并持久化。
- `cluster.listen_address` / `cluster.advertise_address` 用环境变量或启动参数做每节点覆盖（例如 `TOKMESH_CLUSTER_LISTEN_ADDRESS`、`TOKMESH_CLUSTER_ADVERTISE_ADDRESS`），从而保持同一份 `config.yaml` 可复制分发。
- 证书路径可保持一致（如 `./certs/node.crt`），但每个节点必须使用自己签发的证书/私钥，且 SAN 覆盖该节点实际地址。

约束（降低误用）：
- 当 `cluster.enabled=true` 时，`cluster.advertise_address` 必须显式配置为非空；不允许依赖"自动探测"来选择对外地址（多网卡/容器环境误选概率高且难排障）。

### 3.2 节点启动流程

1.  **加载配置**: 解析 `config.yaml` 或环境变量。
2.  **Cluster ID 校验**:
    *   如果本地已持久化 `cluster_id`：启动时加载。
    *   如果未持久化：在后续引导过程中获取或生成。
3.  **连接种子**: 尝试连接 `cluster.discovery.seeds` 中的其他节点。
4.  **判断集群状态**:

    *   **场景 A: 加入现有集群**
        *   如果通过 Gossip 发现种子节点已属于一个活跃集群（具有 `cluster_id`）。
        *   本地 `cluster_id` 必须与发现的集群 ID **一致**。若不一致，则**拒绝加入并告警**。
        *   发起加入请求，获取最新的 Raft 快照和 Shard Map。

    *   **场景 B: 全新集群初始化**
        *   如果 `cluster.discovery.seeds` 中的所有节点都未初始化（`cluster_id` 为空或待定）。
        *   所有待初始化的节点进入 **等待状态**，互相通过 Gossip 发现彼此。
        *   当互相发现的节点数量达到 `cluster.bootstrap.expect_nodes`。
        *   某个节点（例如 NodeID 最小的）被选为引导节点，发起 Raft Leader 选举，生成新的 `cluster_id`。

5.  **Raft 启动**:
    *   加入 Raft Group，同步 Raft 日志。
    *   通过 Raft Learner 机制，从 Leader 节点获取最新的元数据快照。
    *   **Learner → Voter 转换**:
        - 当 Learner 节点的 Raft 日志与 Leader 的 Log Index 差距小于阈值 (如 100 条，可配置)。
        - 或 Learner 节点已成功应用最新的快照 (Snapshot)。
        - 满足条件后，Raft Leader 自动将该 Learner 提升为 Voter，使其获得投票权。

### 3.3 节点加入 (Node Join)

1.  新节点启动，连接种子。
2.  通过 Gossip 发现集群成员。
3.  向 Raft Leader 发起加入请求。
4.  Raft Leader 更新 `Cluster Map`，并通过 Raft 日志复制给所有 Follower。
5.  新节点接收 `Cluster Map`，更新本地 Shard Map。
6.  触发 Rebalance 过程。

### 3.4 节点退出 (Node Leave)

1.  **优雅退出**: 节点主动向 Raft Leader 发送退出请求。
    *   Leader 更新 `Cluster Map`。
    *   该节点的分片数据被迁移。
    *   待所有数据迁移完毕，节点安全下线。
2.  **强制退出 (Raft Leader 剔除)**: 通过 `tokmesh-cli` 强制移除故障节点。
    *   Raft Leader 强制更新 `Cluster Map`。
    *   触发 Rebalance，将该故障节点的分片数据迁移至其他节点。

## 4. 数据分片 (Data Sharding)

### 4.1 虚拟节点 (vNodes)

*   **机制**: 引入虚拟节点 (`vNode`) 概念。每个物理节点拥有 N 个 `vNode`。
*   **优势**:
    *   **负载均衡**: vNode 数量远大于物理节点数，可使数据分布更均匀。
    *   **弹性伸缩**: 节点加入/退出时，只需迁移少量 vNode 即可快速重新平衡。
*   **配置**: 每个物理节点对应 `256` 个 `vNode` (默认值)。

### 4.2 一致性哈希环 (Consistent Hashing Ring)

*   **机制**: 基于 `vNode` 构建一致性哈希环。
*   **分片键**: `Hash(SessionID)`，使用 **MurmurHash3** 算法（32位版本）。
*   **路由**: 根据 `Hash(SessionID)` 在哈希环上找到对应的 `vNode`，从而确定 Owner 物理节点。
*   **Shard Map**: 存储 `vNode` 到 `Physical Node ID` 的映射关系，由控制面 (Raft) 维护和分发。

## 5. 数据复制 (Data Replication)

### 5.1 复制因子 (Replication Factor, RF)

*   **配置**: `cluster.data.replication_factor` (Default: **1**)。
*   **RF=1 (无副本 - 默认)**:
    *   **特点**: 极致性能，最低资源消耗。
    *   **容灾**: 节点宕机，该节点数据丢失（可通过 WAL 重启恢复，但如果磁盘损坏则永久丢失）。
*   **RF > 1 (有副本 - 可选)**:
    *   **示例**: **RF=2 (主备)**, **RF=3**。
    *   **机制**: Owner 节点通过 **异步流 (Async Replication)** 将数据复制到 Backup。
    *   **特性**: 优先保障写入性能，但存在秒级 RPO。
    *   **Future**: 可选同步复制 (Sync Replication) 模式，需强制奇数 RF。

### 5.2 副本放置策略 (Replica Placement)

*   **目标**: 确保 Owner 和 Backup 副本不在同一故障域（同一物理机、同一机架、同一可用区）。
*   **机制**: Raft Leader 在分配 `vNode` 和 Backup 副本时，考虑节点的 IP 地址、Rack ID (需配置) 等信息，尽可能分散。

## 6. 数据再平衡 (Rebalancing)

### 6.1 触发条件

*   新节点加入/退出。
*   现有节点故障/恢复。
*   手动触发 (`tokmesh-cli cluster rebalance`)。

### 6.2 流程与优化

1.  **拓扑变更通知**: Raft Leader 通知所有节点 `Shard Map` 发生变更。
2.  **vNode 归属计算**: 各节点计算新的 `vNode` 归属，确定需要迁移的 `vNode` 列表。
3.  **分批迁移**:
    *   **流控**: 搬迁过程分批进行，并通过 `cluster.rebalance.max_rate` (默认 20Mbps) 限制带宽。
    *   **TTL 优化**: 扫描待搬迁数据时，跳过剩余 TTL 低于 `cluster.rebalance.min_ttl` (默认 60s) 的 Session。
4.  **数据传输**:
    *   **所有权转移（两阶段切换）**:
        1.  **阶段 1 - 双写模式**: 旧 Owner 继续处理该 vNode 的读写请求，同时将写入数据实时同步给新 Owner。
        2.  **阶段 2 - 切换确认**:
            - 新 Owner 确认已接收完整数据（快照 + 增量）。
            - Raft Leader 更新 Shard Map，标记新 Owner 为正式 Owner。
            - 旧 Owner 停止处理该 vNode 的请求，返回重定向错误 (MOVED/TryAgain)，引导客户端或代理层请求新 Owner。
    *   **读兼容**: 迁移期间，对该 `vNode` 的读请求，若新 Owner 无数据，则代理到旧 Owner。
    *   **增量同步**: 采用快照 + 增量日志方式传输数据。

## 7. 故障检测与容错 (Fault Detection & Tolerance)

### 7.1 节点健康检测 (Gossip)

*   **机制**: 各节点通过 Gossip 协议周期性交换心跳信息，探测彼此存活状态。
*   **状态**: `Alive`, `Suspect`, `Dead`。
*   **触发**: 当节点长时间处于 `Suspect` 状态，Gossip 会将其标记为 `Dead`，并通知控制面。

### 7.2 Raft Leader 选举

*   **机制**: 当 Leader 故障，Follower 会超时并触发新的 Leader 选举。
*   **速度**: 通常在几百毫秒到几秒内完成。

### 7.3 Primary-Backup 切换

*   **机制**: 当 Owner 节点故障，其 Backup 节点被提升为新的 Owner。
*   **接管速度**: 受限于 Gossip 协议的故障检测周期及 Raft Leader 对拓扑变更的传播速度，通常在秒级。
*   **数据丢失窗口**: 在异步复制模式下，存在 RPO > 0 的窗口。具体 RPO 值取决于 Owner 节点宕机前异步复制的滞后程度，通常在毫秒到秒级。

### 7.4 脑裂防护 (Split-Brain Protection)

*   **Cluster ID 校验**: 阻止不同集群意外合并。
*   **Raft 多数派**: 确保只有多数派集群能继续写入元数据。
*   **偶数节点告警**: 提示集群配置风险。

## 8. 运维接口 (Admin API)

### 8.1 tokmesh-cli 集群命令

*   **客户端行为**: `tokmesh-cli` 可以连接集群内的任意节点（通过其 HTTP Admin API 端口）。

*   **请求转发**: 接收到管理请求的节点，如果自身不是 Raft Leader，则会自动将请求转发给当前的 Raft Leader 进行处理（例如：创建 Key、触发 Rebalance）。对于本节点特有的操作（例如：查看本地 WAL 日志），则本地处理。

*   `cluster status`: 查看集群健康状态，`Shard Map`，节点列表，`Under-replicated` 告警。

*   `cluster rebalance`: 手动触发 Rebalance。
*   `cluster leave`: 节点优雅退出。
*   `cluster reset`: 清空节点数据和 Cluster ID，用于脑裂恢复。
*   `cluster bootstrap`: 手动触发全新集群初始化 (可选，用于替代 `bootstrap_expect`)。

### 8.2 集群错误码 (Phase 3)

集群操作失败时应返回以下错误码（引用 `specs/governance/error-codes.md`）：

| 场景 | 错误码 | 描述 |
|------|--------|------|
| 无可用节点 | `TM-CLST-5030` | 503 - 所有节点均不可用（用于路由层） |
| 集群操作超时 | `TM-CLST-5040` | 504 - Raft 提交超时、数据迁移超时等 |
| 数据不一致 | `TM-CLST-5050` | 500 - 分片数据丢失、副本校验失败 |
| 节点 ID 冲突 | `TM-CLST-4090` | 409 - 相同 NodeID 的节点尝试加入集群 |

> **Note**: 集群功能在 Phase 3 实现，上述错误码作为预留规划。

---

## 9. 验收标准 (Acceptance Criteria)

### 9.1 功能性验收
参见 `RQ-0401-分布式集群架构.md` 中的所有验收标准。

### 9.2 设计实现验收
以下标准用于验证本设计文档中的具体实现细节：

#### 9.2.1 Raft 控制面
- [ ] **Raft Group 初始化**: 3 节点集群启动，验证 Raft Leader 选举成功，所有节点状态为 Voter。
- [ ] **Learner 机制**: 新节点加入时，初始状态为 Learner。数据同步完成后，自动转为 Voter。
- [ ] **快照压缩**: Raft 日志达到 10000 条后，自动触发 Snapshot，旧日志被清理。
- [ ] **线性一致性读**: 使用 ReadIndex 机制，验证读取到的数据为最新已提交状态。

#### 9.2.2 一致性哈希与分片
- [ ] **vNode 分布均匀**: 256 个 vNode 分布在哈希环上，相邻 vNode 间距方差 < 10%。
- [ ] **路由正确性**: 对同一 SessionID 的请求，在拓扑不变时，始终路由到相同的 Owner 节点。
- [ ] **Hash 函数**: 使用 MurmurHash3 算法计算 `Hash(SessionID)`。

#### 9.2.3 数据迁移
- [ ] **所有权平滑转移**: 迁移期间，读请求不丢失（新 Owner 无数据时代理到旧 Owner）。
- [ ] **流控生效**: Rebalance 过程中，网络带宽占用不超过配置的 `cluster.rebalance.max_rate`。
- [ ] **TTL 优化**: 剩余 TTL 低于 `cluster.rebalance.min_ttl` 的 Session 不被迁移。

#### 9.2.4 副本放置
- [ ] **故障域分散**: RF=2 时，Owner 和 Backup 不在同一 IP 地址（理想情况下不在同一 Rack）。
- [ ] **Under-replicated 检测**: Owner 宕机后，Backup 提升为新 Owner。若无其他节点可用，触发 Under-replicated 告警。

#### 9.2.5 运维接口
- [ ] **请求转发**: `tokmesh-cli` 连接非 Leader 节点，管理命令自动转发到 Leader 执行。
- [ ] **cluster status**: 显示所有节点状态、Shard Map、Under-replicated 分片列表。
