# AD-0102 - 并发 Map 选型决策

**状态**: 已接受
**决策者**: Gemini
**日期**: 2025-12-12
**技术领域**: 后端 / 存储 / 性能
**相关文档**: DS-0101-核心数据模型设计.md
**替代**: 无
**被替代**: 无

## 1. 背景 (Context)

TokMesh 的核心功能是管理高并发的 Session 数据。根据非功能性需求 (RQ-0402-性能与可靠性需求.md)，单节点需支持：
- **容量**: 100 万活跃 Session。
- **吞吐量**: ≥ 10,000 TPS (混合读写)。
- **延迟**: P99 < 1ms。

Session 数据的访问模式具有以下特征：
1.  **读写混合**: `ValidateToken` (读+写/Touch), `CreateSession` (写), `RenewSession` (写)。虽然读多于写，但写入比例不可忽视（尤其是更新 `LastActive`）。
2.  **高并发**: 多线程同时访问不同的 Session。
3.  **内存驻留**: 所有活跃 Session 均存储在内存中。

我们需要选择一种高效的内存数据结构来存储 Session 对象，以满足上述性能要求。

## 2. 备选方案 (Options)

### 选项 A: 标准 `sync.RWMutex` + `map[string]*Session`
- **描述**: Go 语言最基础的并发安全 Map 实现。
- **优点**: 简单，无第三方依赖，内存开销小。
- **缺点**: **锁粒度太粗**。全局一把锁，高并发写入时（即使是不同的 Key），写锁会阻塞所有读操作，导致严重的锁竞争和延迟抖动。

### 选项 B: Go 标准库 `sync.Map`
- **描述**: Go 1.9+ 引入的并发 Map，采用读写分离（read map + dirty map）策略。
- **优点**: 标准库支持，无需额外依赖；在**读多写少**且 key 稳定的场景下性能极佳。
- **缺点**:
    - **写入性能抖动**: 当 dirty map 提升为 read map 时，需要全量拷贝（amorted cost）。
    - **场景不匹配**: `sync.Map` 官方文档明确指出其适用于 "entry 仅被写入一次但被多次读取" 或 "多个 goroutine 读写不相交的 key 集合" 的场景。TokMesh 的 Session 会频繁更新（Renew/Touch），且 Key 集合随用户登录登出动态变化，可能导致 dirty map 频繁提升，性能下降。

### 选项 C: 分段锁 ConcurrentMap (Sharded Lock)
- **描述**: 将一个大 Map 切分为 N 个小 Map (Shards)，每个 Shard 有独立的锁。通过 `Hash(Key) % N` 路由到对应 Shard。
- **优点**:
    - **降低锁粒度**: 锁竞争概率降低为原来的 1/N。
    - **读写均衡**: 在高并发读写混合场景下，性能表现最稳定。
    - **可预测性**: 没有 `sync.Map` 那种复杂的内部状态转换带来的性能抖动。
- **缺点**: 实现比标准 Map 稍复杂；不支持跨 Shard 的原子操作（如 `Len()` 需要锁所有 Shard）。

## 3. 决策 (Decision)

我们决定采用 **选项 C: 分段锁 ConcurrentMap**，并设定默认分片数 (Shard Count) 为 **16** 或 **32**。

**具体实现路径**:
- Phase 1 优先实现轻量级的内部版本（分段锁 + stdlib 哈希），以满足依赖治理要求（见 `specs/adrs/AD-0103-依赖包治理与白名单.md`）。
- 若后续确需引入第三方并发 Map 库，必须走“例外流程”并新增 ADR 更新白名单后方可落地。

## 4. 详细理由 (Justification)

1.  **性能稳定性**: 针对 10,000 TPS 的目标，分段锁能将锁竞争控制在极低水平（< 5%）。相比之下，`sync.Map` 在频繁更新 Session 状态（如 `LastActive`）时可能会遇到不可预测的延迟峰值。
2.  **避免全局阻塞**: 在 Session 驱逐（TTL Cleanup）或快照生成时，我们可以逐个锁定 Shard 进行处理，而不需要暂停整个服务，这对于保证 P99 延迟至关重要。
3.  **对齐架构**: TokMesh 的分布式架构本身就是基于 Consistent Hashing 的分片设计。内存层面的分段锁与业务层面的数据分片概念一致，易于理解和调优。

## 5. 后果 (Consequences)

-   **正向**:
    -   由锁竞争导致的 CPU 上下文切换减少。
    -   高并发写入时的延迟更加平滑。
-   **负向**:
    -   需要引入新的依赖或维护额外的基础代码。
    -   如果未来需要强一致的全局操作（如“获取所有 Session 列表”），实现会比全局锁稍复杂（需要按顺序锁定所有 Shard 或接受弱一致性快照）。
