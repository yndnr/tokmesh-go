# PT-06 一种基于虚拟节点的分布式数据再平衡系统及流控迁移方法

**专利编号**: PT-06
**技术领域**: 分布式系统架构
**创新性评估**: 中
**关联文档**: RQ-0401, DS-0401
**状态**: 草稿
**创建日期**: 2025-12-18

---

## 一、技术领域

本发明涉及分布式系统技术领域，具体涉及一种基于虚拟节点的分布式数据再平衡系统及流控迁移方法，适用于需要动态扩缩容的分布式缓存和存储系统。

---

## 二、背景技术

### 2.1 现有技术描述

在分布式系统中，当集群节点发生变化（新增或移除节点）时，需要重新分配数据以保持负载均衡。常见的数据分布方案包括：

1. **取模分片**：hash(key) % N，N为节点数
2. **一致性哈希**：环形哈希空间，数据分配到顺时针最近的节点
3. **虚拟节点**：每个物理节点对应多个虚拟节点，提高分布均匀性

### 2.2 现有技术的缺陷

1. **取模分片的问题**：节点数变化时，几乎所有数据需要重新分配，数据迁移量巨大。

2. **传统一致性哈希的问题**：
   - 节点分布不均匀，可能出现热点
   - 节点变化时，仍有较大比例数据需要迁移

3. **数据迁移的挑战**：
   - 迁移期间可能影响正常业务
   - 网络带宽被大量占用
   - 缺乏流量控制，可能导致网络拥塞
   - 迁移过程中数据一致性难以保证

4. **会话数据的特殊性**：
   - 会话有TTL，即将过期的数据迁移是浪费
   - 迁移期间需要保证会话可用性
   - 不能丢失活跃会话

---

## 三、发明内容

### 3.1 要解决的技术问题

本发明要解决的技术问题是：如何在节点扩缩容时，最小化数据迁移量，同时通过流量控制和智能策略减少迁移对业务的影响。

### 3.2 技术方案

本发明提供一种基于虚拟节点的分布式数据再平衡系统及流控迁移方法，包括：

#### 3.2.1 虚拟节点设计

**虚拟节点映射**：
```
物理节点 → 虚拟节点映射

Node A (物理) → vNode 0, 256, 512, 768, ...  (每个物理节点256个vNode)
Node B (物理) → vNode 1, 257, 513, 769, ...
Node C (物理) → vNode 2, 258, 514, 770, ...
...

总虚拟节点数: 65536
每物理节点虚拟节点数: 256

数据路由:
hash(key) % 65536 → vNode编号 → 物理节点
```

**节点变化时的影响范围**：
```
场景：3节点集群扩容到4节点

扩容前：
  Node A: 21845 vNodes (33.3%)
  Node B: 21845 vNodes (33.3%)
  Node C: 21846 vNodes (33.4%)

扩容后：
  Node A: 16384 vNodes (25%)
  Node B: 16384 vNodes (25%)
  Node C: 16384 vNodes (25%)
  Node D: 16384 vNodes (25%)  ← 新节点

数据迁移量：
  每个旧节点迁移约 1/4 的数据到新节点
  总迁移量 ≈ 25%（而非传统取模的 75%）
```

#### 3.2.2 系统架构

```
┌─────────────────────────────────────────────────────────────┐
│                    再平衡协调器                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  迁移计划生成器                       │   │
│  │  输入: 旧ShardMap, 新ShardMap                        │   │
│  │  输出: 迁移任务列表 [(srcNode, dstNode, vNodes)]     │   │
│  └─────────────────────────────────────────────────────┘   │
│                              │                              │
│                              ▼                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  智能过滤器                           │   │
│  │  - 跳过即将过期数据 (TTL < 60s)                      │   │
│  │  - 优先迁移活跃会话                                   │   │
│  │  - 按数据大小排序                                     │   │
│  └─────────────────────────────────────────────────────┘   │
│                              │                              │
│                              ▼                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  流量控制器                           │   │
│  │  - 带宽限制: 默认 20 Mbps                            │   │
│  │  - 并发连接数: 默认 4                                │   │
│  │  - 自适应调节: 根据网络状况动态调整                   │   │
│  └─────────────────────────────────────────────────────┘   │
│                              │                              │
│                              ▼                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  迁移执行器                           │   │
│  │  - 双写模式: 迁移期间旧节点继续服务                   │   │
│  │  - 增量同步: 迁移期间的新写入同步到新节点             │   │
│  │  - 原子切换: 迁移完成后原子更新路由                   │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

#### 3.2.3 迁移计划生成

```
FUNCTION generateMigrationPlan(oldMap, newMap):
    migrations = []

    FOR EACH vNode IN 0..65535:
        oldOwner = oldMap.GetOwner(vNode)
        newOwner = newMap.GetOwner(vNode)

        IF oldOwner != newOwner THEN
            migrations.append(MigrationTask{
                VNode:     vNode,
                SourceNode: oldOwner,
                TargetNode: newOwner,
            })
        END IF
    END FOR

    // 按源节点分组，便于批量迁移
    RETURN groupBySource(migrations)
```

#### 3.2.4 智能过滤策略

**TTL过滤**：
```
FUNCTION shouldMigrate(session):
    remainingTTL = session.ExpiresAt - now()

    IF remainingTTL < 60s THEN
        // 即将过期，不值得迁移
        RETURN false
    END IF

    IF remainingTTL < 300s AND session.LastAccessTime < now() - 60s THEN
        // 5分钟内过期且1分钟未访问，可能已不活跃
        RETURN false
    END IF

    RETURN true
```

**优先级排序**：
```
FUNCTION prioritizeSessions(sessions):
    // 按优先级排序
    SORT sessions BY:
        1. LastAccessTime DESC  // 最近访问的优先
        2. ExpiresAt DESC       // 过期时间远的优先
        3. DataSize ASC         // 小数据优先（减少延迟）

    RETURN sessions
```

#### 3.2.5 流量控制机制

**令牌桶算法**：
```
RateLimiter {
    BandwidthLimit: 20 * 1024 * 1024  // 20 Mbps
    BucketSize:     1 * 1024 * 1024   // 1 MB 突发容量
    TokensPerSec:   20 * 1024 * 1024  // 每秒恢复令牌数
}

FUNCTION sendWithRateLimit(data):
    dataSize = len(data)

    // 等待足够的令牌
    WHILE tokens < dataSize:
        refillTokens()
        sleep(10ms)
    END WHILE

    // 消耗令牌
    tokens -= dataSize

    // 发送数据
    send(data)
```

**自适应调节**：
```
FUNCTION adjustBandwidth():
    // 监控指标
    currentLatency = measureLatency()
    currentQPS = measureQPS()

    IF currentLatency > targetLatency * 1.5 THEN
        // 延迟过高，降低迁移带宽
        bandwidthLimit *= 0.8
    ELSE IF currentLatency < targetLatency * 0.8 AND bandwidthLimit < maxBandwidth THEN
        // 延迟正常，可以提高迁移带宽
        bandwidthLimit *= 1.1
    END IF
```

#### 3.2.6 双写模式

```
迁移期间的请求处理：

┌──────────────────────────────────────────────────────────────┐
│                       客户端请求                              │
└──────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌──────────────────────────────────────────────────────────────┐
│                       路由层                                  │
│  检查 vNode 状态:                                            │
│  - NORMAL: 直接路由到 Owner                                  │
│  - MIGRATING: 进入双写模式                                   │
└──────────────────────────────────────────────────────────────┘
                              │
              ┌───────────────┴───────────────┐
              │ MIGRATING 状态                 │
              ▼                               ▼
┌─────────────────────────┐     ┌─────────────────────────┐
│      Source Node        │     │      Target Node        │
│  (旧 Owner)             │     │  (新 Owner)             │
├─────────────────────────┤     ├─────────────────────────┤
│ 读请求: 本地处理         │     │ 读请求: 转发到 Source   │
│ 写请求: 本地写入         │────▶│ 写请求: 同步写入        │
│         + 转发到 Target │     │                         │
└─────────────────────────┘     └─────────────────────────┘

切换完成后:
┌─────────────────────────┐     ┌─────────────────────────┐
│      Source Node        │     │      Target Node        │
│  (不再持有该 vNode)      │     │  (新 Owner, 独立服务)   │
└─────────────────────────┘     └─────────────────────────┘
```

### 3.3 有益效果

1. **最小化迁移量**：
   - 虚拟节点机制使节点变化只影响部分数据
   - 扩容一个节点，迁移量约为 1/N（N为新节点数）

2. **业务无感知**：
   - 双写模式保证迁移期间服务可用
   - 流量控制避免影响正常业务延迟

3. **智能优化**：
   - 跳过即将过期数据，减少无效迁移
   - 优先迁移活跃数据，保证服务质量

4. **网络友好**：
   - 令牌桶限流，避免网络拥塞
   - 自适应调节，平衡迁移速度和业务性能

5. **一致性保障**：
   - 双写模式保证数据不丢失
   - 原子切换避免数据重复或遗漏

---

## 四、具体实施方式

### 4.1 实施例1：虚拟节点映射

```go
type VNodeMapper struct {
    totalVNodes   uint32
    vnodesPerNode uint32
    nodeVNodes    map[string][]uint32  // nodeID -> vNodes
}

func NewVNodeMapper(totalVNodes, vnodesPerNode uint32) *VNodeMapper {
    return &VNodeMapper{
        totalVNodes:   totalVNodes,
        vnodesPerNode: vnodesPerNode,
        nodeVNodes:    make(map[string][]uint32),
    }
}

// 分配虚拟节点给物理节点
func (m *VNodeMapper) AssignVNodes(nodes []string) map[uint32]string {
    shardMap := make(map[uint32]string)
    nodeCount := len(nodes)

    // 均匀分配虚拟节点
    for vnode := uint32(0); vnode < m.totalVNodes; vnode++ {
        nodeIndex := int(vnode) % nodeCount
        nodeID := nodes[nodeIndex]
        shardMap[vnode] = nodeID
        m.nodeVNodes[nodeID] = append(m.nodeVNodes[nodeID], vnode)
    }

    return shardMap
}

// 计算扩容后的新映射
func (m *VNodeMapper) RebalanceForExpansion(oldMap map[uint32]string, newNode string) map[uint32]string {
    newMap := make(map[uint32]string)
    nodeCount := len(m.nodeVNodes) + 1
    vnodesPerNode := m.totalVNodes / uint32(nodeCount)

    // 每个旧节点贡献一部分 vNode 给新节点
    vnodesToMigrate := int(vnodesPerNode)
    migratedCount := 0

    for vnode, oldOwner := range oldMap {
        if migratedCount < vnodesToMigrate && shouldMigrateVNode(vnode, oldOwner) {
            newMap[vnode] = newNode
            migratedCount++
        } else {
            newMap[vnode] = oldOwner
        }
    }

    return newMap
}
```

### 4.2 实施例2：迁移计划生成

```go
type MigrationTask struct {
    VNode      uint32
    SourceNode string
    TargetNode string
    Priority   int
    Sessions   []*Session
}

type MigrationPlanner struct {
    ttlThreshold time.Duration
}

func (p *MigrationPlanner) GeneratePlan(oldMap, newMap map[uint32]string) []*MigrationTask {
    tasks := make([]*MigrationTask, 0)

    for vnode := uint32(0); vnode < 65536; vnode++ {
        oldOwner := oldMap[vnode]
        newOwner := newMap[vnode]

        if oldOwner != newOwner {
            tasks = append(tasks, &MigrationTask{
                VNode:      vnode,
                SourceNode: oldOwner,
                TargetNode: newOwner,
            })
        }
    }

    // 按源节点分组
    return p.groupAndPrioritize(tasks)
}

func (p *MigrationPlanner) FilterSessions(sessions []*Session) []*Session {
    filtered := make([]*Session, 0, len(sessions))

    for _, sess := range sessions {
        if p.shouldMigrate(sess) {
            filtered = append(filtered, sess)
        }
    }

    // 按优先级排序
    sort.Slice(filtered, func(i, j int) bool {
        return filtered[i].LastAccessTime.After(filtered[j].LastAccessTime)
    })

    return filtered
}

func (p *MigrationPlanner) shouldMigrate(sess *Session) bool {
    remainingTTL := sess.ExpiresAt.Sub(time.Now())

    // 跳过即将过期的会话
    if remainingTTL < p.ttlThreshold {
        return false
    }

    return true
}
```

### 4.3 实施例3：流量控制器

```go
type RateLimiter struct {
    mu            sync.Mutex
    tokens        int64
    maxTokens     int64
    tokensPerSec  int64
    lastRefill    time.Time
}

func NewRateLimiter(bandwidthMbps int) *RateLimiter {
    bytesPerSec := int64(bandwidthMbps * 1024 * 1024 / 8)
    return &RateLimiter{
        tokens:       bytesPerSec,
        maxTokens:    bytesPerSec,
        tokensPerSec: bytesPerSec,
        lastRefill:   time.Now(),
    }
}

func (r *RateLimiter) Acquire(bytes int64) {
    r.mu.Lock()
    defer r.mu.Unlock()

    // 补充令牌
    r.refill()

    // 等待足够的令牌
    for r.tokens < bytes {
        r.mu.Unlock()
        time.Sleep(10 * time.Millisecond)
        r.mu.Lock()
        r.refill()
    }

    // 消耗令牌
    r.tokens -= bytes
}

func (r *RateLimiter) refill() {
    now := time.Now()
    elapsed := now.Sub(r.lastRefill)
    tokensToAdd := int64(elapsed.Seconds() * float64(r.tokensPerSec))

    if tokensToAdd > 0 {
        r.tokens = min(r.tokens+tokensToAdd, r.maxTokens)
        r.lastRefill = now
    }
}

// 自适应调节
func (r *RateLimiter) Adjust(currentLatency, targetLatency time.Duration) {
    r.mu.Lock()
    defer r.mu.Unlock()

    if currentLatency > targetLatency*3/2 {
        // 延迟过高，降低带宽
        r.tokensPerSec = r.tokensPerSec * 80 / 100
    } else if currentLatency < targetLatency*4/5 {
        // 延迟正常，提高带宽
        r.tokensPerSec = min(r.tokensPerSec*110/100, r.maxTokens)
    }
}
```

### 4.4 实施例4：双写迁移执行器

```go
type MigrationExecutor struct {
    rateLimiter *RateLimiter
    sourceConn  *grpc.ClientConn
    targetConn  *grpc.ClientConn
}

func (e *MigrationExecutor) Execute(task *MigrationTask) error {
    // 1. 标记 vNode 为迁移中
    e.setVNodeStatus(task.VNode, StatusMigrating)

    // 2. 获取要迁移的会话列表
    sessions, err := e.fetchSessionsFromSource(task)
    if err != nil {
        return err
    }

    // 3. 逐个迁移会话
    for _, sess := range sessions {
        // 流量控制
        e.rateLimiter.Acquire(int64(sess.Size()))

        // 写入目标节点
        if err := e.writeToTarget(task.TargetNode, sess); err != nil {
            return err
        }

        // 标记已迁移
        e.markMigrated(sess.ID)
    }

    // 4. 切换 vNode 所有权
    e.setVNodeStatus(task.VNode, StatusNormal)
    e.updateShardMap(task.VNode, task.TargetNode)

    // 5. 清理源节点数据
    e.cleanupSource(task)

    return nil
}

// 双写模式下的写请求处理
func (e *MigrationExecutor) HandleWriteDuringMigration(vnode uint32, session *Session) error {
    status := e.getVNodeStatus(vnode)

    if status == StatusMigrating {
        // 双写：同时写入源节点和目标节点
        errChan := make(chan error, 2)

        go func() {
            errChan <- e.writeToSource(session)
        }()

        go func() {
            errChan <- e.writeToTarget(e.getTargetNode(vnode), session)
        }()

        // 等待两个写入完成
        for i := 0; i < 2; i++ {
            if err := <-errChan; err != nil {
                return err
            }
        }
    } else {
        // 正常模式：只写入当前 Owner
        return e.writeToOwner(vnode, session)
    }

    return nil
}
```

---

## 五、权利要求书

### 权利要求1（独立权利要求 - 系统）

一种基于虚拟节点的分布式数据再平衡系统，其特征在于，包括：

**虚拟节点映射模块**，用于将数据哈希空间划分为多个虚拟节点，并将虚拟节点映射到物理节点，每个物理节点对应多个虚拟节点；

**迁移计划生成模块**，用于在节点扩缩容时，比较新旧分片映射，生成虚拟节点迁移任务列表，包含源节点、目标节点和待迁移的虚拟节点信息；

**智能过滤模块**，用于过滤不需要迁移的数据，包括：
- 剩余生存时间小于预定阈值的数据；
- 长时间未被访问的数据；
并按访问时间和过期时间对待迁移数据进行优先级排序；

**流量控制模块**，用于限制数据迁移的网络带宽消耗，采用令牌桶算法控制数据传输速率，并根据系统负载自适应调节迁移带宽；

**迁移执行模块**，用于执行数据迁移，采用双写模式保证迁移期间数据的可用性和一致性。

### 权利要求2（从属权利要求）

根据权利要求1所述的系统，其特征在于，所述虚拟节点映射模块配置总虚拟节点数为65536，每个物理节点对应256个虚拟节点，节点扩缩容时仅需迁移约1/N的数据（N为新节点数）。

### 权利要求3（从属权利要求）

根据权利要求1所述的系统，其特征在于，所述智能过滤模块的过滤阈值为60秒，即剩余生存时间小于60秒的数据不进行迁移。

### 权利要求4（从属权利要求）

根据权利要求1所述的系统，其特征在于，所述流量控制模块的默认带宽限制为20Mbps，并根据当前系统延迟动态调节：延迟超过目标值1.5倍时降低带宽，延迟低于目标值0.8倍时提高带宽。

### 权利要求5（从属权利要求）

根据权利要求1所述的系统，其特征在于，所述迁移执行模块的双写模式包括：迁移期间写请求同时写入源节点和目标节点，读请求由源节点处理，迁移完成后原子切换路由到目标节点。

### 权利要求6（独立权利要求 - 方法）

一种基于虚拟节点的分布式数据流控迁移方法，其特征在于，包括以下步骤：

**S1：迁移计划生成步骤**，比较节点变更前后的分片映射，识别所有权发生变化的虚拟节点，生成迁移任务列表；

**S2：数据过滤步骤**，对待迁移数据进行过滤：
- 跳过剩余生存时间小于预定阈值的数据；
- 按访问时间降序排列，优先迁移活跃数据；

**S3：带宽控制步骤**，使用令牌桶算法控制数据传输速率：
- 发送数据前获取相应数量的令牌；
- 令牌不足时等待令牌恢复；

**S4：双写迁移步骤**，执行数据迁移：
- 标记虚拟节点为迁移中状态；
- 迁移期间写请求同时写入源节点和目标节点；
- 数据迁移完成后原子切换路由；

**S5：清理步骤**，迁移完成后清理源节点上的已迁移数据。

### 权利要求7（从属权利要求）

根据权利要求6所述的方法，其特征在于，所述步骤S3还包括自适应调节子步骤，根据系统当前延迟动态调整令牌恢复速率。

### 权利要求8（从属权利要求）

根据权利要求6所述的方法，其特征在于，所述步骤S4中，读请求在迁移期间由源节点处理，写请求由源节点处理后转发到目标节点同步。

---

## 六、说明书附图

### 图1：虚拟节点分布示意图

```
哈希空间 (0 - 65535)
────────────────────────────────────────────────────────────►

3节点集群:
┌────────────────┬────────────────┬────────────────┐
│    Node A      │    Node B      │    Node C      │
│  21845 vNodes  │  21845 vNodes  │  21846 vNodes  │
│    (33.3%)     │    (33.3%)     │    (33.4%)     │
└────────────────┴────────────────┴────────────────┘

扩容到4节点:
┌────────────┬────────────┬────────────┬────────────┐
│   Node A   │   Node B   │   Node C   │   Node D   │
│ 16384 vN   │ 16384 vN   │ 16384 vN   │ 16384 vN   │
│   (25%)    │   (25%)    │   (25%)    │   (25%)    │
└────────────┴────────────┴────────────┴────────────┘

迁移量分析:
┌─────────────────────────────────────────────────────────┐
│ Node A: 21845 → 16384，迁移出 5461 vNodes (25%)         │
│ Node B: 21845 → 16384，迁移出 5461 vNodes (25%)         │
│ Node C: 21846 → 16384，迁移出 5462 vNodes (25%)         │
│ Node D: 0 → 16384，接收 16384 vNodes (新节点)            │
│                                                         │
│ 总迁移量: 16384 vNodes = 25% 数据                       │
│ (对比取模分片: ~75% 数据需要迁移)                        │
└─────────────────────────────────────────────────────────┘
```

### 图2：双写迁移流程图

```
时间线
──────────────────────────────────────────────────────────────►
T0                T1              T2              T3

│                 │               │               │
│  开始迁移       │  迁移中       │  切换完成     │  清理完成
│                 │               │               │
▼                 ▼               ▼               ▼

状态: NORMAL      MIGRATING       NORMAL          NORMAL
Owner: A          A (主) + D (副)  D               D

┌─────────────────────────────────────────────────────────┐
│ T0-T1: 准备阶段                                          │
│   - 计算迁移计划                                         │
│   - 标记 vNode 为 MIGRATING                             │
│   - 通知 Target 节点准备接收                             │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ T1-T2: 迁移阶段 (双写模式)                               │
│                                                         │
│   读请求处理:                                            │
│   Client ──► Source (A) ──► Response                   │
│                                                         │
│   写请求处理:                                            │
│   Client ──► Source (A) ──┬──► Local Write            │
│                           └──► Forward to Target (D)   │
│                                                         │
│   数据迁移:                                              │
│   Source (A) ══► [流量控制] ══► Target (D)             │
│                                                         │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ T2: 原子切换                                             │
│   - 更新 ShardMap: vNode Owner = D                      │
│   - 标记 vNode 为 NORMAL                                │
│   - 所有请求路由到 Target (D)                            │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ T2-T3: 清理阶段                                          │
│   - Source (A) 异步删除已迁移数据                        │
│   - 释放资源                                             │
└─────────────────────────────────────────────────────────┘
```

---

## 七、摘要

本发明公开了一种基于虚拟节点的分布式数据再平衡系统及流控迁移方法。该系统将数据哈希空间划分为65536个虚拟节点，每个物理节点对应256个虚拟节点。节点扩缩容时，通过比较新旧分片映射生成迁移计划，仅迁移所有权变化的虚拟节点数据。系统采用智能过滤策略跳过即将过期的数据，使用令牌桶算法控制迁移带宽并根据系统负载自适应调节。迁移期间采用双写模式，写请求同时写入源节点和目标节点，保证数据一致性和服务可用性。本发明解决了传统分布式系统扩缩容时数据迁移量大、影响业务性能的问题，将迁移量从约75%降低到约25%，同时通过流量控制和智能优化最小化对业务的影响。

**关键词**：虚拟节点；数据再平衡；流量控制；双写模式；分布式系统
