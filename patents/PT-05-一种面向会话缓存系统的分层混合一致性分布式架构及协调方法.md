# PT-05 一种面向会话缓存系统的分层混合一致性分布式架构及协调方法

**专利编号**: PT-05
**技术领域**: 分布式系统架构
**创新性评估**: 高
**关联文档**: RQ-0401, DS-0401
**状态**: 草稿
**创建日期**: 2025-12-18

---

## 一、技术领域

本发明涉及分布式系统技术领域，具体涉及一种面向会话缓存系统的分层混合一致性分布式架构及协调方法，适用于需要在高吞吐量和数据一致性之间取得平衡的会话管理系统。

---

## 二、背景技术

### 2.1 现有技术描述

在分布式系统中，一致性协议的选择直接影响系统的性能和可靠性。常见的一致性方案包括：

1. **强一致性（Raft/Paxos）**：所有操作通过共识协议同步，保证线性一致性。
2. **最终一致性（Gossip）**：节点间异步传播更新，最终达到一致状态。
3. **一致性哈希**：数据按哈希值分布到不同节点，支持动态扩缩容。

### 2.2 现有技术的缺陷

1. **全Raft方案的问题**：
   - 所有操作都经过共识协议，写入延迟高（跨节点RTT）
   - Leader节点成为性能瓶颈
   - 不适合高吞吐量场景（如10万+ TPS）

2. **纯最终一致性方案的问题**：
   - 关键元数据（如集群配置）可能出现不一致
   - 节点间数据冲突难以解决
   - 安全敏感数据（如API Key）需要强一致保障

3. **单一一致性模型的局限**：
   - 不同类型数据对一致性的要求不同
   - 元数据需要强一致，业务数据需要高吞吐
   - 现有系统难以兼顾

4. **会话缓存系统的特殊需求**：
   - 会话数据高频读写，需要低延迟
   - 会话撤销需要快速传播
   - 集群配置变更需要强一致

---

## 三、发明内容

### 3.1 要解决的技术问题

本发明要解决的技术问题是：如何设计一种分层的混合一致性架构，使不同类型的数据采用最适合的一致性协议，在保证关键数据强一致性的同时，实现业务数据的高吞吐量处理。

### 3.2 技术方案

本发明提供一种面向会话缓存系统的分层混合一致性分布式架构及协调方法，包括：

#### 3.2.1 三层架构设计

```
┌─────────────────────────────────────────────────────────────┐
│                      分层混合一致性架构                       │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                   控制面 (Control Plane)              │   │
│  │                                                       │   │
│  │  一致性协议: Raft (强一致)                            │   │
│  │  数据类型:                                            │   │
│  │    - Cluster Map (集群拓扑)                          │   │
│  │    - Shard Map (分片映射)                            │   │
│  │    - API Keys (密钥元数据)                           │   │
│  │    - 配置信息                                        │   │
│  │                                                       │   │
│  │  特点: 写入少、一致性要求高、不可丢失                  │   │
│  └─────────────────────────────────────────────────────┘   │
│                              │                              │
│                              │ 读取分片映射                  │
│                              ▼                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                   数据面 (Data Plane)                 │   │
│  │                                                       │   │
│  │  路由策略: 一致性哈希 (高吞吐)                         │   │
│  │  数据类型:                                            │   │
│  │    - Session (会话数据)                              │   │
│  │    - Token Hash Index (令牌索引)                     │   │
│  │                                                       │   │
│  │  特点: 高频读写、可容忍短暂不一致、支持弹性伸缩        │   │
│  └─────────────────────────────────────────────────────┘   │
│                              │                              │
│                              │ 状态变更事件                  │
│                              ▼                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                   监测面 (Monitor Plane)              │   │
│  │                                                       │   │
│  │  传播协议: Gossip (弱一致)                            │   │
│  │  数据类型:                                            │   │
│  │    - 节点健康状态                                     │   │
│  │    - 会话撤销事件                                     │   │
│  │    - 负载信息                                         │   │
│  │                                                       │   │
│  │  特点: 广播式传播、容忍部分丢失、P99 < 2s             │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 3.2.2 控制面设计（Raft协议）

**管理的数据**：
| 数据类型 | 说明 | 一致性要求 |
|----------|------|------------|
| Cluster Map | 集群节点列表、角色 | 强一致 |
| Shard Map | 数据分片到节点的映射 | 强一致 |
| API Keys | 密钥元数据（不含密文） | 强一致 |
| 系统配置 | 全局配置参数 | 强一致 |

**Raft配置**：
```
RaftConfig {
    ElectionTimeout:  150-300ms    // 选举超时
    HeartbeatInterval: 50ms        // 心跳间隔
    SnapshotThreshold: 10000       // 快照触发条目数
    MaxLogEntries:     100000      // 日志条目上限
}
```

**操作流程**：
```
写操作：
1. Client → Leader 发送写请求
2. Leader 追加日志条目
3. Leader 并行复制到 Follower
4. 多数节点确认后提交
5. 应用到状态机
6. 返回 Client

读操作（线性读）：
1. Client → Leader 发送读请求
2. Leader 确认自己仍是 Leader（心跳确认）
3. 读取状态机数据
4. 返回 Client
```

#### 3.2.3 数据面设计（一致性哈希）

**分片策略**：
```
ShardingStrategy {
    TotalVNodes:      65536       // 总虚拟节点数
    VNodesPerNode:    256         // 每物理节点虚拟节点数
    ReplicationFactor: 2          // 副本因子
    HashFunction:     xxHash64    // 哈希函数
}
```

**路由算法**：
```
FUNCTION routeRequest(key):
    // 计算哈希值
    hash = xxHash64(key)

    // 确定虚拟节点
    vnode = hash % TotalVNodes

    // 查询Shard Map获取物理节点
    // Shard Map从控制面Raft读取
    nodeID = ShardMap.GetOwner(vnode)

    RETURN nodeID
```

**数据操作流程**：
```
写操作：
1. 计算 key 的哈希值
2. 通过 Shard Map 确定目标节点
3. 直接写入目标节点（无需共识）
4. 可选：异步复制到副本节点
5. 返回成功

读操作：
1. 计算 key 的哈希值
2. 通过 Shard Map 确定目标节点
3. 直接从目标节点读取
4. 返回数据
```

#### 3.2.4 监测面设计（Gossip协议）

**传播的信息**：
| 信息类型 | 传播频率 | 时效性要求 |
|----------|----------|------------|
| 节点心跳 | 1s | 5s内检测故障 |
| 会话撤销 | 事件触发 | P99 < 2s |
| 负载信息 | 5s | 10s内可见 |
| 版本信息 | 变更时 | 1min内收敛 |

**Gossip配置**：
```
GossipConfig {
    FanOut:           3           // 每轮传播节点数
    Interval:         200ms       // 传播间隔
    SuspectTimeout:   5s          // 疑似故障超时
    DeadTimeout:      30s         // 确认死亡超时
    RetransmitMult:   4           // 重传乘数
}
```

**传播算法**：
```
FUNCTION gossipRound():
    // 选择随机节点传播
    targets = selectRandomNodes(FanOut)

    // 构建传播消息
    message = GossipMessage{
        NodeID:    self.ID,
        Heartbeat: self.Heartbeat,
        Events:    pendingEvents,
        Version:   self.StateVersion,
    }

    // 发送给目标节点
    FOR EACH target IN targets:
        send(target, message)

    // 更新本地心跳
    self.Heartbeat++
```

#### 3.2.5 跨层协调机制

**分片变更流程**（控制面 → 数据面）：
```
1. 管理员触发节点扩容/缩容
2. 控制面 Raft 提议新的 Shard Map
3. 多数节点确认，提交变更
4. 各节点从控制面读取新 Shard Map
5. 数据面开始 Rebalance 迁移数据
6. 迁移完成后，新 Shard Map 生效
```

**会话撤销流程**（数据面 → 监测面 → 数据面）：
```
1. 数据面收到会话撤销请求
2. 本地删除会话数据
3. 发布撤销事件到监测面 Gossip
4. Gossip 广播到所有节点
5. 各节点收到事件后删除本地缓存
6. 传播窗口 P99 < 2s
```

**节点故障处理流程**（监测面 → 控制面 → 数据面）：
```
1. 监测面 Gossip 检测到节点无心跳
2. 超过 SuspectTimeout，标记为疑似故障
3. 超过 DeadTimeout，标记为已死亡
4. 向控制面报告节点故障
5. 控制面 Raft 更新 Cluster Map
6. 控制面 Raft 更新 Shard Map（重新分配分片）
7. 数据面执行故障恢复（从副本恢复数据）
```

### 3.3 有益效果

1. **性能优化**：
   - 业务数据通过一致性哈希直接路由，无需经过共识协议
   - 单节点吞吐量可达 10万+ TPS
   - 读写延迟 P99 < 10ms（局域网）

2. **一致性保障**：
   - 关键元数据（Shard Map、API Keys）通过 Raft 保证强一致
   - 避免脑裂和数据冲突

3. **高可用性**：
   - Gossip 协议快速检测节点故障
   - 自动故障转移和数据恢复
   - 无单点故障

4. **弹性伸缩**：
   - 一致性哈希支持平滑扩缩容
   - Shard Map 变更通过 Raft 保证一致
   - 数据迁移对业务透明

5. **事件传播**：
   - 会话撤销通过 Gossip 快速广播
   - P99 传播延迟 < 2s
   - 资源消耗低

---

## 四、具体实施方式

### 4.1 实施例1：控制面Raft实现

```go
// Raft 状态机
type ControlPlaneStateMachine struct {
    clusterMap *ClusterMap
    shardMap   *ShardMap
    apiKeys    *APIKeyStore
    config     *SystemConfig
}

// 应用 Raft 日志条目
func (sm *ControlPlaneStateMachine) Apply(entry *raft.LogEntry) interface{} {
    var cmd Command
    if err := proto.Unmarshal(entry.Data, &cmd); err != nil {
        return err
    }

    switch cmd.Type {
    case CmdUpdateClusterMap:
        return sm.applyClusterMapUpdate(cmd.Payload)
    case CmdUpdateShardMap:
        return sm.applyShardMapUpdate(cmd.Payload)
    case CmdCreateAPIKey:
        return sm.applyCreateAPIKey(cmd.Payload)
    case CmdRevokeAPIKey:
        return sm.applyRevokeAPIKey(cmd.Payload)
    default:
        return ErrUnknownCommand
    }
}

// 提议变更（通过 Leader）
func (cp *ControlPlane) ProposeShardMapChange(newMap *ShardMap) error {
    cmd := Command{
        Type:    CmdUpdateShardMap,
        Payload: proto.Marshal(newMap),
    }

    // 通过 Raft 共识
    future := cp.raft.Apply(proto.Marshal(cmd), 5*time.Second)
    return future.Error()
}

// 读取 Shard Map（线性读）
func (cp *ControlPlane) GetShardMap() (*ShardMap, error) {
    // 确保读取最新数据
    if err := cp.raft.VerifyLeader().Error(); err != nil {
        // 转发到 Leader
        return cp.forwardToLeader("GetShardMap")
    }

    return cp.stateMachine.shardMap.Clone(), nil
}
```

### 4.2 实施例2：数据面一致性哈希

```go
// 一致性哈希路由器
type ConsistentHashRouter struct {
    shardMap    *ShardMap
    hashFunc    func([]byte) uint64
    totalVNodes uint32
}

func NewConsistentHashRouter(shardMap *ShardMap) *ConsistentHashRouter {
    return &ConsistentHashRouter{
        shardMap:    shardMap,
        hashFunc:    xxhash.Sum64,
        totalVNodes: 65536,
    }
}

// 路由请求到目标节点
func (r *ConsistentHashRouter) Route(key string) (nodeID string, err error) {
    // 计算哈希
    hash := r.hashFunc([]byte(key))

    // 确定虚拟节点
    vnode := uint32(hash % uint64(r.totalVNodes))

    // 查询 Shard Map
    nodeID, exists := r.shardMap.GetOwner(vnode)
    if !exists {
        return "", ErrNoNodeForVNode
    }

    return nodeID, nil
}

// 数据面操作
type DataPlane struct {
    router      *ConsistentHashRouter
    localStore  *SessionStore
    nodeID      string
    connections map[string]*grpc.ClientConn
}

// 写入会话
func (dp *DataPlane) SetSession(session *Session) error {
    // 路由到目标节点
    targetNode, err := dp.router.Route(session.ID)
    if err != nil {
        return err
    }

    if targetNode == dp.nodeID {
        // 本地写入
        return dp.localStore.Set(session)
    } else {
        // 转发到目标节点
        return dp.forwardSet(targetNode, session)
    }
}

// 读取会话
func (dp *DataPlane) GetSession(sessionID string) (*Session, error) {
    targetNode, err := dp.router.Route(sessionID)
    if err != nil {
        return nil, err
    }

    if targetNode == dp.nodeID {
        return dp.localStore.Get(sessionID)
    } else {
        return dp.forwardGet(targetNode, sessionID)
    }
}
```

### 4.3 实施例3：监测面Gossip实现

```go
// Gossip 成员管理
type GossipMemberlist struct {
    config      *memberlist.Config
    memberlist  *memberlist.Memberlist
    eventChan   chan GossipEvent
    broadcasts  *memberlist.TransmitLimitedQueue
}

// 初始化 Gossip
func NewGossipMemberlist(nodeID string, bindAddr string) (*GossipMemberlist, error) {
    config := memberlist.DefaultWANConfig()
    config.Name = nodeID
    config.BindAddr = bindAddr
    config.GossipInterval = 200 * time.Millisecond
    config.ProbeInterval = 1 * time.Second
    config.SuspicionMult = 4

    g := &GossipMemberlist{
        config:    config,
        eventChan: make(chan GossipEvent, 100),
    }

    // 设置事件代理
    config.Events = g

    ml, err := memberlist.Create(config)
    if err != nil {
        return nil, err
    }
    g.memberlist = ml

    return g, nil
}

// 广播会话撤销事件
func (g *GossipMemberlist) BroadcastSessionRevoke(sessionID string) {
    event := &SessionRevokeEvent{
        SessionID: sessionID,
        Timestamp: time.Now(),
        SourceNode: g.config.Name,
    }

    data, _ := proto.Marshal(event)
    broadcast := &simpleBroadcast{
        msg:    data,
        notify: nil,
    }

    g.broadcasts.QueueBroadcast(broadcast)
}

// 节点状态变更回调
func (g *GossipMemberlist) NotifyJoin(node *memberlist.Node) {
    g.eventChan <- GossipEvent{
        Type: EventNodeJoin,
        Node: node.Name,
    }
}

func (g *GossipMemberlist) NotifyLeave(node *memberlist.Node) {
    g.eventChan <- GossipEvent{
        Type: EventNodeLeave,
        Node: node.Name,
    }
}
```

### 4.4 实施例4：跨层协调器

```go
// 跨层协调器
type LayerCoordinator struct {
    controlPlane *ControlPlane
    dataPlane    *DataPlane
    monitorPlane *GossipMemberlist
}

// 处理节点故障
func (c *LayerCoordinator) HandleNodeFailure(failedNodeID string) error {
    // 1. 监测面已检测到故障

    // 2. 更新控制面
    currentMap, _ := c.controlPlane.GetClusterMap()
    newMap := currentMap.Clone()
    newMap.RemoveNode(failedNodeID)

    if err := c.controlPlane.ProposeClusterMapChange(newMap); err != nil {
        return err
    }

    // 3. 重新计算 Shard Map
    currentShardMap, _ := c.controlPlane.GetShardMap()
    newShardMap := c.rebalanceShardMap(currentShardMap, newMap)

    if err := c.controlPlane.ProposeShardMapChange(newShardMap); err != nil {
        return err
    }

    // 4. 数据面开始恢复数据
    return c.dataPlane.RecoverFromReplicas(failedNodeID)
}

// 处理会话撤销
func (c *LayerCoordinator) RevokeSession(sessionID string) error {
    // 1. 数据面删除本地数据
    targetNode, _ := c.dataPlane.router.Route(sessionID)
    if err := c.dataPlane.DeleteSession(sessionID); err != nil {
        return err
    }

    // 2. 通过 Gossip 广播撤销事件
    c.monitorPlane.BroadcastSessionRevoke(sessionID)

    return nil
}

// Shard Map 变更通知
func (c *LayerCoordinator) OnShardMapChange(newMap *ShardMap) {
    // 更新数据面路由器
    c.dataPlane.router.UpdateShardMap(newMap)

    // 触发数据迁移
    go c.dataPlane.StartRebalance(newMap)
}
```

---

## 五、权利要求书

### 权利要求1（独立权利要求 - 系统）

一种面向会话缓存系统的分层混合一致性分布式架构，其特征在于，包括三个功能层：

**控制面**，采用Raft共识协议管理集群元数据，包括：
- 集群拓扑信息（Cluster Map）；
- 数据分片映射（Shard Map）；
- API密钥元数据；
- 系统配置信息；
所述控制面保证元数据的强一致性和线性一致性读写；

**数据面**，采用一致性哈希策略处理业务数据，包括：
- 会话数据的存储和检索；
- 令牌哈希索引的维护；
所述数据面根据所述控制面提供的分片映射进行数据路由，支持高吞吐量的读写操作；

**监测面**，采用Gossip协议进行状态传播，包括：
- 节点健康状态检测；
- 会话撤销事件广播；
- 负载信息收集；
所述监测面支持事件的快速广播，传播延迟P99小于预定阈值；

以及**跨层协调模块**，用于协调三个功能层之间的交互，处理节点故障、分片变更和会话撤销等跨层操作。

### 权利要求2（从属权利要求）

根据权利要求1所述的架构，其特征在于，所述控制面的Raft协议配置包括：选举超时时间150-300毫秒，心跳间隔50毫秒，快照触发阈值10000条日志条目。

### 权利要求3（从属权利要求）

根据权利要求1所述的架构，其特征在于，所述数据面的一致性哈希策略采用虚拟节点机制，总虚拟节点数为65536，每个物理节点对应256个虚拟节点，支持平滑的弹性伸缩。

### 权利要求4（从属权利要求）

根据权利要求1所述的架构，其特征在于，所述监测面的Gossip协议配置包括：每轮传播节点数为3，传播间隔200毫秒，疑似故障超时5秒，确认死亡超时30秒。

### 权利要求5（从属权利要求）

根据权利要求1所述的架构，其特征在于，所述跨层协调模块在处理节点故障时依次执行：监测面检测故障、更新控制面集群拓扑、重新计算分片映射、数据面执行数据恢复。

### 权利要求6（独立权利要求 - 方法）

一种面向会话缓存系统的分层混合一致性协调方法，其特征在于，包括以下步骤：

**S1：分层部署步骤**，在分布式系统中部署三个功能层：
- 控制面：采用Raft协议管理元数据；
- 数据面：采用一致性哈希处理业务数据；
- 监测面：采用Gossip协议传播状态；

**S2：元数据管理步骤**，通过控制面的Raft协议对集群拓扑和分片映射进行强一致性管理：
- 所有元数据变更通过Raft Leader提议；
- 多数节点确认后提交变更；
- 各节点应用变更到本地状态机；

**S3：数据路由步骤**，数据面根据控制面的分片映射进行数据路由：
- 计算数据键的哈希值；
- 通过分片映射确定目标节点；
- 直接向目标节点发起读写请求；

**S4：状态传播步骤**，监测面通过Gossip协议传播状态信息：
- 定期广播节点心跳；
- 事件触发时广播变更事件；
- 各节点接收并处理传播的信息；

**S5：跨层协调步骤**，在发生跨层事件时进行协调处理：
- 节点故障：监测面检测 → 控制面更新 → 数据面恢复；
- 分片变更：控制面提议 → 数据面迁移；
- 会话撤销：数据面删除 → 监测面广播。

### 权利要求7（从属权利要求）

根据权利要求6所述的方法，其特征在于，所述步骤S3中，数据面的读写操作不经过Raft共识协议，直接与目标节点通信，实现高吞吐量数据处理。

### 权利要求8（从属权利要求）

根据权利要求6所述的方法，其特征在于，所述步骤S4中，会话撤销事件的传播延迟P99小于2秒。

### 权利要求9（从属权利要求）

根据权利要求6所述的方法，其特征在于，所述步骤S5中的节点故障处理包括：监测面在预定超时后将节点标记为死亡，向控制面报告，控制面通过Raft更新集群拓扑和分片映射，数据面从副本恢复故障节点的数据。

### 权利要求10（从属权利要求）

根据权利要求6所述的方法，其特征在于，数据面从控制面读取分片映射时采用缓存机制，仅在分片映射版本变更时更新本地缓存，减少对控制面的访问压力。

---

## 六、说明书附图

### 图1：三层架构示意图

```
┌─────────────────────────────────────────────────────────────────┐
│                         TokMesh 集群                             │
│                                                                  │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                     控制面 (Raft)                           │ │
│  │  ┌──────────┐    ┌──────────┐    ┌──────────┐              │ │
│  │  │  Node A  │◄──►│  Node B  │◄──►│  Node C  │              │ │
│  │  │ (Leader) │    │(Follower)│    │(Follower)│              │ │
│  │  └──────────┘    └──────────┘    └──────────┘              │ │
│  │       │                                                     │ │
│  │       │ Raft 日志复制                                       │ │
│  │       ▼                                                     │ │
│  │  [Cluster Map] [Shard Map] [API Keys] [Config]             │ │
│  │                                                             │ │
│  │  特点: 强一致 | 低吞吐 | 关键元数据                          │ │
│  └────────────────────────────────────────────────────────────┘ │
│                              │                                   │
│                              │ 读取 Shard Map                    │
│                              ▼                                   │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                     数据面 (一致性哈希)                      │ │
│  │                                                             │ │
│  │     ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐               │ │
│  │     │Shard│ │Shard│ │Shard│ │Shard│ │Shard│  ...          │ │
│  │     │  0  │ │  1  │ │  2  │ │  3  │ │  4  │               │ │
│  │     └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘               │ │
│  │        │       │       │       │       │                   │ │
│  │     Node A  Node B  Node C  Node A  Node B                 │ │
│  │                                                             │ │
│  │  路由: hash(key) % 65536 → vNode → Node                    │ │
│  │                                                             │ │
│  │  特点: 高吞吐 | 低延迟 | 业务数据                            │ │
│  └────────────────────────────────────────────────────────────┘ │
│                              │                                   │
│                              │ 事件广播                          │
│                              ▼                                   │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                     监测面 (Gossip)                         │ │
│  │                                                             │ │
│  │     ┌──────┐      ┌──────┐      ┌──────┐                   │ │
│  │     │Node A│─────►│Node B│─────►│Node C│                   │ │
│  │     └──┬───┘      └──┬───┘      └──┬───┘                   │ │
│  │        │             │             │                        │ │
│  │        └─────────────┴─────────────┘                        │ │
│  │              Gossip 随机传播                                 │ │
│  │                                                             │ │
│  │  传播: 心跳 | 撤销事件 | 负载信息                            │ │
│  │                                                             │ │
│  │  特点: 弱一致 | 低开销 | P99 < 2s                           │ │
│  └────────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────────┘
```

### 图2：跨层协调流程图

```
场景：节点 C 发生故障

时间线：
────────────────────────────────────────────────────────────────►

T0: 节点 C 停止响应
    │
    ▼
┌─────────────────────────────────────────────────────┐
│                    监测面 (Gossip)                   │
│  T0+1s: Node A, B 检测到 C 无心跳                   │
│  T0+5s: C 标记为 Suspect (疑似故障)                 │
│  T0+30s: C 标记为 Dead (确认死亡)                   │
│          → 向控制面报告                              │
└───────────────────────────┬─────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────┐
│                    控制面 (Raft)                     │
│  T0+30s: Leader 收到故障报告                        │
│  T0+30.1s: 提议更新 Cluster Map (移除 C)            │
│  T0+30.2s: Follower 确认                            │
│  T0+30.3s: 提议更新 Shard Map (重新分配 C 的分片)   │
│  T0+30.4s: Follower 确认                            │
│            → 通知数据面                              │
└───────────────────────────┬─────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────┐
│                    数据面 (一致性哈希)               │
│  T0+30.5s: 收到新 Shard Map                         │
│  T0+30.5s~T0+60s: 从副本恢复 C 的数据               │
│  T0+60s: 恢复完成，新 Shard Map 生效                │
└─────────────────────────────────────────────────────┘

总故障恢复时间: ~60s (可配置)
```

---

## 七、摘要

本发明公开了一种面向会话缓存系统的分层混合一致性分布式架构及协调方法。该架构将分布式系统分为三个功能层：控制面采用Raft共识协议管理集群拓扑、分片映射等关键元数据，保证强一致性；数据面采用一致性哈希策略处理会话等业务数据，实现高吞吐量（10万+TPS）的读写；监测面采用Gossip协议传播节点健康状态和会话撤销事件，传播延迟P99<2s。跨层协调模块处理节点故障、分片变更等跨层事件。本发明解决了单一一致性模型难以同时满足元数据强一致和业务数据高吞吐需求的问题，为会话缓存系统提供了一种高性能、高可用、强一致的分布式解决方案。

**关键词**：分布式架构；混合一致性；Raft；一致性哈希；Gossip；会话缓存
